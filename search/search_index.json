{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Documentation for my HomeLab GitOps repository \u2014 a single Kubernetes cluster managed declaratively with ArgoCD on bare-metal Talos Linux. Architecture Single node amd64 server running Talos Linux v1.9.5 2x 1TB HDDs : one for persistent storage (OpenEBS LVM), one for backups (Minio) GitOps : ArgoCD auto-syncs this repository to the cluster Secrets : HashiCorp Vault + External Secrets Operator + ArgoCD Vault Plugin Technology Stack Component Tool OS Talos Linux CNI Cilium (L2/ARP mode, no kube-proxy) GitOps ArgoCD with Vault Plugin Ingress Traefik TLS cert-manager (Cloudflare DNS-01) DNS External DNS (Cloudflare) Storage OpenEBS LVM Backup Volsync + Restic \u2192 local Minio Secrets Vault + External Secrets Monitoring Prometheus + Grafana + Loki Repository Layout cluster/ # Single cluster definition patches/ # Talos machine config patches bootstrap/ # ArgoCD bootstrap apps + ApplicationSets system/ # Infrastructure components (storage, DNS, monitoring, etc.) argocd/ # ArgoCD install & config (also bootstrapped via extraManifests) cilium/ # Cilium CNI install (bootstrapped via extraManifests) apps/ # User applications (managed via ApplicationSet) docs/ # This documentation Getting Started See the Getting Started guide for installation instructions.","title":"Welcome"},{"location":"#welcome","text":"Documentation for my HomeLab GitOps repository \u2014 a single Kubernetes cluster managed declaratively with ArgoCD on bare-metal Talos Linux.","title":"Welcome"},{"location":"#architecture","text":"Single node amd64 server running Talos Linux v1.9.5 2x 1TB HDDs : one for persistent storage (OpenEBS LVM), one for backups (Minio) GitOps : ArgoCD auto-syncs this repository to the cluster Secrets : HashiCorp Vault + External Secrets Operator + ArgoCD Vault Plugin","title":"Architecture"},{"location":"#technology-stack","text":"Component Tool OS Talos Linux CNI Cilium (L2/ARP mode, no kube-proxy) GitOps ArgoCD with Vault Plugin Ingress Traefik TLS cert-manager (Cloudflare DNS-01) DNS External DNS (Cloudflare) Storage OpenEBS LVM Backup Volsync + Restic \u2192 local Minio Secrets Vault + External Secrets Monitoring Prometheus + Grafana + Loki","title":"Technology Stack"},{"location":"#repository-layout","text":"cluster/ # Single cluster definition patches/ # Talos machine config patches bootstrap/ # ArgoCD bootstrap apps + ApplicationSets system/ # Infrastructure components (storage, DNS, monitoring, etc.) argocd/ # ArgoCD install & config (also bootstrapped via extraManifests) cilium/ # Cilium CNI install (bootstrapped via extraManifests) apps/ # User applications (managed via ApplicationSet) docs/ # This documentation","title":"Repository Layout"},{"location":"#getting-started","text":"See the Getting Started guide for installation instructions.","title":"Getting Started"},{"location":"backup/","text":"Backup System Backups use Volsync to replicate PVCs via Restic to a local Minio instance running on the second HDD. Architecture PVC \u2192 Volsync ReplicationSource \u2192 Restic \u2192 Minio (hostPath /var/minio-data on 2nd HDD) Minio runs as a Deployment with a hostPath volume at /var/minio-data (see cluster/system/minio/ ) Volsync is installed via Helm chart (see cluster/system/storage/volsync.yaml ) Restic credentials are stored in Vault and synced via ExternalSecret (see cluster/system/backup/restic-credentials.yaml ) Restic Credentials in Vault Store credentials at kv/restic in Vault: vault kv put kv/restic \\ AWS_ACCESS_KEY_ID=\"minioadmin\" \\ AWS_SECRET_ACCESS_KEY=\"your-minio-password\" \\ RESTIC_PASSWORD=\"your-restic-encryption-password\" \\ RESTIC_REPOSITORY=\"s3:http://minio.minio.svc.cluster.local:9000/backups\" The ExternalSecret in cluster/system/backup/restic-credentials.yaml syncs these to a Kubernetes Secret named restic-base-credentials in the volsync-system namespace. Initialize a Restic Repository Before creating your first backup, initialize the restic repository in Minio: kubectl port-forward -n minio svc/minio 9000:9000 & export AWS_ACCESS_KEY_ID=minioadmin export AWS_SECRET_ACCESS_KEY=your-minio-password export RESTIC_PASSWORD=your-restic-encryption-password export RESTIC_REPOSITORY=s3:http://localhost:9000/backups restic init Create a Backup (ReplicationSource) To back up a PVC, create a ReplicationSource in the same namespace: apiVersion: volsync.backube/v1alpha1 kind: ReplicationSource metadata: name: my-app-backup namespace: my-app spec: sourcePVC: my-app-data trigger: schedule: \"0 */6 * * *\" restic: pruneIntervalDays: 7 repository: restic-credentials retain: hourly: 6 daily: 5 weekly: 4 monthly: 2 yearly: 1 copyMethod: Direct A commented example is at cluster/system/backup/example-volsync.yaml . Verify Backups kubectl port-forward -n minio svc/minio 9000:9000 & restic snapshots Restore from Backup Create a ReplicationDestination : apiVersion: volsync.backube/v1alpha1 kind: ReplicationDestination metadata: name: my-app-restore namespace: my-app spec: trigger: manual: restore-once restic: repository: restic-credentials destinationPVC: my-app-data copyMethod: Direct","title":"Backup"},{"location":"backup/#backup-system","text":"Backups use Volsync to replicate PVCs via Restic to a local Minio instance running on the second HDD.","title":"Backup System"},{"location":"backup/#architecture","text":"PVC \u2192 Volsync ReplicationSource \u2192 Restic \u2192 Minio (hostPath /var/minio-data on 2nd HDD) Minio runs as a Deployment with a hostPath volume at /var/minio-data (see cluster/system/minio/ ) Volsync is installed via Helm chart (see cluster/system/storage/volsync.yaml ) Restic credentials are stored in Vault and synced via ExternalSecret (see cluster/system/backup/restic-credentials.yaml )","title":"Architecture"},{"location":"backup/#restic-credentials-in-vault","text":"Store credentials at kv/restic in Vault: vault kv put kv/restic \\ AWS_ACCESS_KEY_ID=\"minioadmin\" \\ AWS_SECRET_ACCESS_KEY=\"your-minio-password\" \\ RESTIC_PASSWORD=\"your-restic-encryption-password\" \\ RESTIC_REPOSITORY=\"s3:http://minio.minio.svc.cluster.local:9000/backups\" The ExternalSecret in cluster/system/backup/restic-credentials.yaml syncs these to a Kubernetes Secret named restic-base-credentials in the volsync-system namespace.","title":"Restic Credentials in Vault"},{"location":"backup/#initialize-a-restic-repository","text":"Before creating your first backup, initialize the restic repository in Minio: kubectl port-forward -n minio svc/minio 9000:9000 & export AWS_ACCESS_KEY_ID=minioadmin export AWS_SECRET_ACCESS_KEY=your-minio-password export RESTIC_PASSWORD=your-restic-encryption-password export RESTIC_REPOSITORY=s3:http://localhost:9000/backups restic init","title":"Initialize a Restic Repository"},{"location":"backup/#create-a-backup-replicationsource","text":"To back up a PVC, create a ReplicationSource in the same namespace: apiVersion: volsync.backube/v1alpha1 kind: ReplicationSource metadata: name: my-app-backup namespace: my-app spec: sourcePVC: my-app-data trigger: schedule: \"0 */6 * * *\" restic: pruneIntervalDays: 7 repository: restic-credentials retain: hourly: 6 daily: 5 weekly: 4 monthly: 2 yearly: 1 copyMethod: Direct A commented example is at cluster/system/backup/example-volsync.yaml .","title":"Create a Backup (ReplicationSource)"},{"location":"backup/#verify-backups","text":"kubectl port-forward -n minio svc/minio 9000:9000 & restic snapshots","title":"Verify Backups"},{"location":"backup/#restore-from-backup","text":"Create a ReplicationDestination : apiVersion: volsync.backube/v1alpha1 kind: ReplicationDestination metadata: name: my-app-restore namespace: my-app spec: trigger: manual: restore-once restic: repository: restic-credentials destinationPVC: my-app-data copyMethod: Direct","title":"Restore from Backup"},{"location":"cloudflared/","text":"Expose Applications with Cloudflare Tunnel (Optional) Note : This setup is not currently active. It's documented here for reference in case you want to expose services through Cloudflare Tunnel in the future instead of (or alongside) direct access. Cloudflare Tunnel creates a secure outbound connection from the cluster to Cloudflare's edge, allowing you to expose services without opening inbound ports. This is free with any Cloudflare plan. Cloudflare Install cloudflared, this is the client that will create the tunnel between the cluster and Cloudflare. You can still use the web interface to create the tunnel, but it's easier to manage it with the CLI. brew install cloudflared cloudflared login # Login to your Cloudflare account Create a tunnel with the CLI, note the ID of the tunnel that will be used later. cloudflared tunnel create my-cluster # Save the tunnel ID and credentials JSON file By creating the tunnel, you obtain a JSON file with the credentials that will be used to authenticate the tunnel with Cloudflare, keep it safe and do not commit it to the repository (or encrypt it if you do, even if it's still not recommended). kubectl create namespace cloudflare kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=/path/to/<TUNNEL_ID>.json \\ --namespace=cloudflare Install the Cloudflare Tunnel Controller in the cloudflare namespace. values.yaml cloudflare: tunnelName: \"my-cluster\" tunnelId: \"<TUNNEL_ID>\" secretName: \"tunnel-credentials\" ingress: - hostname: \"*.yourdomain.com\" service: \"https://traefik.traefik.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 helm repo add cloudflare https://cloudflare.github.io/helm-charts helm repo update helm upgrade --install cloudflare-tunnel cloudflare/cloudflare-tunnel \\ --namespace cloudflare \\ --values values.yaml ArgoCD Application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cloudflare-tunnel namespace: argocd spec: project: default source: repoURL: https://cloudflare.github.io/helm-charts chart: cloudflare-tunnel targetRevision: 0.3.2 helm: values: | cloudflare: tunnelName: \"my-cluster\" tunnelId: \"<TUNNEL_ID>\" secretName: \"cloudflare-tunnel\" ingress: - hostname: \"*.yourdomain.com\" service: \"https://traefik.traefik.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 destination: server: https://kubernetes.default.svc namespace: cloudflare syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true If you create a CNAME record in Cloudflare pointing to <TUNNEL_ID>.cfargotunnel.com , you can access the services on the cluster using the domain name. External DNS External DNS is already configured in cluster/system/external-dns/ and will automatically create DNS records in Cloudflare for each Ingress resource. Create an API key in Cloudflare that can edit your DNS records of the domain (Zone) you configured in Cloudflare. Store the token in Vault: vault kv put kv/cloudflare dnsToken=\"your-cloudflare-api-token\" The ExternalSecret in cluster/system/external-dns/external-secret.yaml syncs this to a Kubernetes Secret automatically. Now, you can add annotations to your Ingress resources to automatically create DNS records in Cloudflare. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: external-dns.alpha.kubernetes.io/hostname: my-app.yourdomain.com name: my-app namespace: default spec: ingressClassName: traefik rules: - host: my-app.yourdomain.com http: paths: - backend: service: name: my-app port: number: 80 path: / pathType: Prefix tls: - hosts: - my-app.yourdomain.com secretName: my-app-tls If using Cloudflare Tunnel, add the target annotation: external-dns.alpha.kubernetes.io/target: <TUNNEL_ID>.cfargotunnel.com external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" The External DNS ArgoCD application is already defined in cluster/system/external-dns/external-dns.yaml .","title":"Cloudflared"},{"location":"cloudflared/#expose-applications-with-cloudflare-tunnel-optional","text":"Note : This setup is not currently active. It's documented here for reference in case you want to expose services through Cloudflare Tunnel in the future instead of (or alongside) direct access. Cloudflare Tunnel creates a secure outbound connection from the cluster to Cloudflare's edge, allowing you to expose services without opening inbound ports. This is free with any Cloudflare plan.","title":"Expose Applications with Cloudflare Tunnel (Optional)"},{"location":"cloudflared/#cloudflare","text":"Install cloudflared, this is the client that will create the tunnel between the cluster and Cloudflare. You can still use the web interface to create the tunnel, but it's easier to manage it with the CLI. brew install cloudflared cloudflared login # Login to your Cloudflare account Create a tunnel with the CLI, note the ID of the tunnel that will be used later. cloudflared tunnel create my-cluster # Save the tunnel ID and credentials JSON file By creating the tunnel, you obtain a JSON file with the credentials that will be used to authenticate the tunnel with Cloudflare, keep it safe and do not commit it to the repository (or encrypt it if you do, even if it's still not recommended). kubectl create namespace cloudflare kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=/path/to/<TUNNEL_ID>.json \\ --namespace=cloudflare Install the Cloudflare Tunnel Controller in the cloudflare namespace. values.yaml cloudflare: tunnelName: \"my-cluster\" tunnelId: \"<TUNNEL_ID>\" secretName: \"tunnel-credentials\" ingress: - hostname: \"*.yourdomain.com\" service: \"https://traefik.traefik.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 helm repo add cloudflare https://cloudflare.github.io/helm-charts helm repo update helm upgrade --install cloudflare-tunnel cloudflare/cloudflare-tunnel \\ --namespace cloudflare \\ --values values.yaml ArgoCD Application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cloudflare-tunnel namespace: argocd spec: project: default source: repoURL: https://cloudflare.github.io/helm-charts chart: cloudflare-tunnel targetRevision: 0.3.2 helm: values: | cloudflare: tunnelName: \"my-cluster\" tunnelId: \"<TUNNEL_ID>\" secretName: \"cloudflare-tunnel\" ingress: - hostname: \"*.yourdomain.com\" service: \"https://traefik.traefik.svc.cluster.local:443\" originRequest: noTLSVerify: true resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" replicaCount: 1 destination: server: https://kubernetes.default.svc namespace: cloudflare syncPolicy: automated: prune: true syncOptions: - CreateNamespace=true If you create a CNAME record in Cloudflare pointing to <TUNNEL_ID>.cfargotunnel.com , you can access the services on the cluster using the domain name.","title":"Cloudflare"},{"location":"cloudflared/#external-dns","text":"External DNS is already configured in cluster/system/external-dns/ and will automatically create DNS records in Cloudflare for each Ingress resource. Create an API key in Cloudflare that can edit your DNS records of the domain (Zone) you configured in Cloudflare. Store the token in Vault: vault kv put kv/cloudflare dnsToken=\"your-cloudflare-api-token\" The ExternalSecret in cluster/system/external-dns/external-secret.yaml syncs this to a Kubernetes Secret automatically. Now, you can add annotations to your Ingress resources to automatically create DNS records in Cloudflare. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: external-dns.alpha.kubernetes.io/hostname: my-app.yourdomain.com name: my-app namespace: default spec: ingressClassName: traefik rules: - host: my-app.yourdomain.com http: paths: - backend: service: name: my-app port: number: 80 path: / pathType: Prefix tls: - hosts: - my-app.yourdomain.com secretName: my-app-tls If using Cloudflare Tunnel, add the target annotation: external-dns.alpha.kubernetes.io/target: <TUNNEL_ID>.cfargotunnel.com external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" The External DNS ArgoCD application is already defined in cluster/system/external-dns/external-dns.yaml .","title":"External DNS"},{"location":"ebs-lvm/","text":"Storage (OpenEBS LVM) This cluster uses OpenEBS LVM LocalPV to provide persistent storage backed by an LVM volume group on the storage HDD. Setup 1. Create the LVM Volume Group After installing Talos and bootstrapping the cluster, create an LVM volume group on the storage disk. Since Talos is immutable, use a privileged debug pod (see Getting Started - Step 4 for the full command): # Inside the debug pod: apt-get update && apt-get install -y lvm2 pvcreate /dev/sdb # Replace with your storage disk vgcreate lvmvg /dev/sdb vgdisplay lvmvg # Verify exit 2. Deploy OpenEBS OpenEBS is deployed via ArgoCD Application in cluster/bootstrap/openebs.yaml . It installs: OpenEBS LVM LocalPV controller and node agent StorageClass openebs-lvmpv (default) backed by the lvmvg volume group # The StorageClass created: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-lvmpv annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: local.csi.openebs.io parameters: storage: \"lvm\" volgroup: \"lvmvg\" volumeBindingMode: WaitForFirstConsumer reclaimPolicy: Retain allowVolumeExpansion: true 3. Use in Applications Simply create a PVC \u2014 it will use the default openebs-lvmpv StorageClass: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-app-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Monitoring Check LVM volume group space: # Via debug pod kubectl run --rm -it lvm-check --image=ubuntu:24.04 --privileged --overrides='...' # Inside: vgs lvmvg Check OpenEBS volumes: kubectl get lvmvolumes -A Notes LVM volumes are node-local \u2014 PVCs are bound to the node where the volume group resides WaitForFirstConsumer binding mode ensures volumes are created on the correct node Volume expansion is supported ( allowVolumeExpansion: true ) For backups, use Volsync \u2014 see Backup","title":"Storage (OpenEBS LVM)"},{"location":"ebs-lvm/#storage-openebs-lvm","text":"This cluster uses OpenEBS LVM LocalPV to provide persistent storage backed by an LVM volume group on the storage HDD.","title":"Storage (OpenEBS LVM)"},{"location":"ebs-lvm/#setup","text":"","title":"Setup"},{"location":"ebs-lvm/#1-create-the-lvm-volume-group","text":"After installing Talos and bootstrapping the cluster, create an LVM volume group on the storage disk. Since Talos is immutable, use a privileged debug pod (see Getting Started - Step 4 for the full command): # Inside the debug pod: apt-get update && apt-get install -y lvm2 pvcreate /dev/sdb # Replace with your storage disk vgcreate lvmvg /dev/sdb vgdisplay lvmvg # Verify exit","title":"1. Create the LVM Volume Group"},{"location":"ebs-lvm/#2-deploy-openebs","text":"OpenEBS is deployed via ArgoCD Application in cluster/bootstrap/openebs.yaml . It installs: OpenEBS LVM LocalPV controller and node agent StorageClass openebs-lvmpv (default) backed by the lvmvg volume group # The StorageClass created: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-lvmpv annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: local.csi.openebs.io parameters: storage: \"lvm\" volgroup: \"lvmvg\" volumeBindingMode: WaitForFirstConsumer reclaimPolicy: Retain allowVolumeExpansion: true","title":"2. Deploy OpenEBS"},{"location":"ebs-lvm/#3-use-in-applications","text":"Simply create a PVC \u2014 it will use the default openebs-lvmpv StorageClass: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-app-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi","title":"3. Use in Applications"},{"location":"ebs-lvm/#monitoring","text":"Check LVM volume group space: # Via debug pod kubectl run --rm -it lvm-check --image=ubuntu:24.04 --privileged --overrides='...' # Inside: vgs lvmvg Check OpenEBS volumes: kubectl get lvmvolumes -A","title":"Monitoring"},{"location":"ebs-lvm/#notes","text":"LVM volumes are node-local \u2014 PVCs are bound to the node where the volume group resides WaitForFirstConsumer binding mode ensures volumes are created on the correct node Volume expansion is supported ( allowVolumeExpansion: true ) For backups, use Volsync \u2014 see Backup","title":"Notes"},{"location":"getting-started/","text":"Getting Started Prerequisites A bare-metal machine (amd64) with 2 HDDs: HDD 1 : Storage disk (OpenEBS LVM) HDD 2 : Backup disk (Minio) A domain managed by Cloudflare talosctl installed on your workstation task installed on your workstation kubectl installed on your workstation Step 1: Get a Talos Image Go to the Talos Image Factory and create a schematic with these extensions: siderolabs/iscsi-tools (required for OpenEBS LVM) siderolabs/util-linux-tools (useful utilities) Download the ISO or raw disk image for your architecture (amd64). Save your Schematic ID \u2014 you'll need it for upgrades. Step 2: Install Talos on the Machine Option A: USB Boot Write the ISO to a USB drive and boot from it: dd if=talos-amd64.iso of=/dev/sdX bs=4M status=progress Option B: Rescue Mode (e.g., OVH) If your server is hosted, boot into rescue mode and write the image directly: wget https://factory.talos.dev/image/<SCHEMATIC_ID>/v1.9.5/metal-amd64.raw.xz xz -d metal-amd64.raw.xz dd if=metal-amd64.raw of=/dev/sda bs=4M status=progress Reboot from disk after writing. Step 3: Configure and Deploy Fork/clone this repo Create your environment file : bash cp .env.example .env # Edit .env with your node IP, cluster name, schematic ID Generate and apply the Talos config : bash task gen-config # Generates controlplane.yaml with all patches task apply-config # Applies config to the node (first time: --insecure) Bootstrap the cluster (first time only): bash task bootstrap # Initializes etcd and the control plane task kubeconfig # Downloads kubeconfig to ~/.kube/config Wait for self-provisioning : Talos boots \u2192 extraManifests install Cilium + ArgoCD + metrics-server ArgoCD reads bootstrap.yml \u2192 deploys cluster/bootstrap/ (ArgoCD config, cert-manager, Vault, ApplicationSets, OpenEBS) ApplicationSets discover and deploy everything else Note : Talos extraManifests installs the upstream ArgoCD manifests directly. Once the bootstrap ArgoCD Application syncs, it replaces them with the kustomized version (which includes AVP sidecars and the server.insecure ConfigMap patch). Step 4: Set Up Storage Storage HDD (OpenEBS LVM) After the cluster is running, create an LVM volume group on the storage disk: # Find your storage disk talosctl get disks --nodes <NODE_IP> --endpoints <NODE_IP> --talosconfig cluster/talos/talosconfig # Create a namespace with privileged PodSecurity (required for host access) kubectl create namespace debug kubectl label namespace debug pod-security.kubernetes.io/enforce=privileged # Create a debug pod with LVM tools and host /dev access # (chroot won't work on Talos \u2014 it has no shell on the host rootfs) kubectl apply -f - <<'EOF' apiVersion: v1 kind: Pod metadata: name: lvm-setup namespace: debug spec: hostPID: true containers: - name: lvm-setup image: ubuntu:24.04 command: [\"bash\", \"-c\", \"apt-get update && apt-get install -y lvm2 && echo READY && sleep 3600\"] securityContext: privileged: true volumeMounts: - name: dev mountPath: /dev - name: run-udev mountPath: /run/udev volumes: - name: dev hostPath: path: /dev - name: run-udev hostPath: path: /run/udev restartPolicy: Never EOF # Wait for lvm2 to be installed kubectl wait --for=condition=Ready pod/lvm-setup -n debug --timeout=180s # Create the LVM physical volume and volume group kubectl exec lvm-setup -n debug -- pvcreate /dev/sdb kubectl exec lvm-setup -n debug -- vgcreate lvmvg /dev/sdb # Clean up kubectl delete pod lvm-setup -n debug kubectl delete namespace debug The OpenEBS ArgoCD Application ( cluster/bootstrap/openebs.yaml ) will create the openebs-lvmpv StorageClass automatically. Backup HDD (Minio) The backup disk is mounted via the Talos patch cluster/patches/backup-disk.yml . Ensure the disk is formatted and the mount path matches: # Format the backup disk (if needed) via debug pod # mkfs.ext4 /dev/sdc # mkdir -p /var/minio-data # mount /dev/sdc /var/minio-data The Minio deployment ( cluster/system/minio/ ) uses a hostPath volume pointing to /var/minio-data . Step 5: Initialize Vault Vault must be initialized and unsealed manually on first boot: # Wait for Vault pod to be running kubectl get pods -n vault -w # Initialize Vault task vault-init # This outputs cluster-keys.json \u2014 SAVE THIS SECURELY # Unseal Vault (joins and unseals all replicas) task vault-unseal # Enable KV v1 secrets engine ROOT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") kubectl exec -n vault vault-0 -- sh -c \"VAULT_TOKEN=$ROOT_TOKEN vault secrets enable -path=kv -version=1 kv\" # Create the Vault token secret for External Secrets # (namespace may not exist yet \u2014 ApplicationSets haven't synced) kubectl create namespace external-secrets --dry-run=client -o yaml | kubectl apply -f - kubectl create secret generic vault-token \\ --namespace external-secrets \\ --from-literal=token=$ROOT_TOKEN Step 6: Create ArgoCD Vault Credentials Create the vault-credentials secret so the AVP sidecars (already deployed via bootstrap) can connect to Vault: # Create the vault-credentials secret for AVP ROOT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") kubectl create secret generic vault-credentials \\ --namespace argocd \\ --from-literal=AVP_AUTH_TYPE=token \\ --from-literal=AVP_KV_VERSION=1 \\ --from-literal=AVP_TYPE=vault \\ --from-literal=VAULT_ADDR=http://vault.vault.svc.cluster.local:8200 \\ --from-literal=VAULT_TOKEN=$ROOT_TOKEN The ArgoCD repo-server is deployed with AVP CMP sidecars from bootstrap. Once this secret is created, the sidecars will start resolving <path:kv/...> placeholders in manifests at sync time. Step 7: Populate Vault Secrets # Port-forward Vault task vault-port-forward & export VAULT_ADDR=http://localhost:8200 export VAULT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") # Cluster basics vault kv put kv/cluster domain=\"yourdomain.com\" IP=\"192.168.1.100\" email=\"you@example.com\" # Cloudflare DNS token vault kv put kv/cloudflare dnsToken=\"your-cloudflare-api-token\" # Grafana vault kv put kv/grafana user=\"admin\" pass=\"your-grafana-password\" \\ prometheus_user=\"prometheus\" prometheus_pass=\"your-prom-password\" # Minio vault kv put kv/minio rootUser=\"minioadmin\" rootPassword=\"your-minio-password\" # Restic backup credentials (use Minio creds) vault kv put kv/restic \\ AWS_ACCESS_KEY_ID=\"minioadmin\" \\ AWS_SECRET_ACCESS_KEY=\"your-minio-password\" \\ RESTIC_PASSWORD=\"your-restic-encryption-password\" \\ RESTIC_REPOSITORY=\"s3:http://minio.minio.svc.cluster.local:9000/backups\" # --- App secrets (add when deploying apps) --- # Kyoo (media server) vault kv put kv/kyoo apikey=\"your-kyoo-api-key\" tmdb=\"your-tmdb-api-key\" # Sonarr vault kv put kv/sonarr \\ token=\"your-sonarr-api-key\" \\ url=\"http://sonarr.sonarr.svc.cluster.local:8989\" # Radarr vault kv put kv/radarr \\ token=\"your-radarr-api-key\" \\ url=\"http://radarr.radarr.svc.cluster.local:7878\" Step 8: Verify # Check cluster health task health # Check ArgoCD applications kubectl get applications -n argocd # Access ArgoCD UI (get initial admin password) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d # Then access via the ingress: https://argocd.<your-domain> Upgrading Talos task upgrade-talos -- v1.10.0 Kubernetes task upgrade-k8s -- v1.33.0","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#prerequisites","text":"A bare-metal machine (amd64) with 2 HDDs: HDD 1 : Storage disk (OpenEBS LVM) HDD 2 : Backup disk (Minio) A domain managed by Cloudflare talosctl installed on your workstation task installed on your workstation kubectl installed on your workstation","title":"Prerequisites"},{"location":"getting-started/#step-1-get-a-talos-image","text":"Go to the Talos Image Factory and create a schematic with these extensions: siderolabs/iscsi-tools (required for OpenEBS LVM) siderolabs/util-linux-tools (useful utilities) Download the ISO or raw disk image for your architecture (amd64). Save your Schematic ID \u2014 you'll need it for upgrades.","title":"Step 1: Get a Talos Image"},{"location":"getting-started/#step-2-install-talos-on-the-machine","text":"","title":"Step 2: Install Talos on the Machine"},{"location":"getting-started/#option-a-usb-boot","text":"Write the ISO to a USB drive and boot from it: dd if=talos-amd64.iso of=/dev/sdX bs=4M status=progress","title":"Option A: USB Boot"},{"location":"getting-started/#option-b-rescue-mode-eg-ovh","text":"If your server is hosted, boot into rescue mode and write the image directly: wget https://factory.talos.dev/image/<SCHEMATIC_ID>/v1.9.5/metal-amd64.raw.xz xz -d metal-amd64.raw.xz dd if=metal-amd64.raw of=/dev/sda bs=4M status=progress Reboot from disk after writing.","title":"Option B: Rescue Mode (e.g., OVH)"},{"location":"getting-started/#step-3-configure-and-deploy","text":"Fork/clone this repo Create your environment file : bash cp .env.example .env # Edit .env with your node IP, cluster name, schematic ID Generate and apply the Talos config : bash task gen-config # Generates controlplane.yaml with all patches task apply-config # Applies config to the node (first time: --insecure) Bootstrap the cluster (first time only): bash task bootstrap # Initializes etcd and the control plane task kubeconfig # Downloads kubeconfig to ~/.kube/config Wait for self-provisioning : Talos boots \u2192 extraManifests install Cilium + ArgoCD + metrics-server ArgoCD reads bootstrap.yml \u2192 deploys cluster/bootstrap/ (ArgoCD config, cert-manager, Vault, ApplicationSets, OpenEBS) ApplicationSets discover and deploy everything else Note : Talos extraManifests installs the upstream ArgoCD manifests directly. Once the bootstrap ArgoCD Application syncs, it replaces them with the kustomized version (which includes AVP sidecars and the server.insecure ConfigMap patch).","title":"Step 3: Configure and Deploy"},{"location":"getting-started/#step-4-set-up-storage","text":"","title":"Step 4: Set Up Storage"},{"location":"getting-started/#storage-hdd-openebs-lvm","text":"After the cluster is running, create an LVM volume group on the storage disk: # Find your storage disk talosctl get disks --nodes <NODE_IP> --endpoints <NODE_IP> --talosconfig cluster/talos/talosconfig # Create a namespace with privileged PodSecurity (required for host access) kubectl create namespace debug kubectl label namespace debug pod-security.kubernetes.io/enforce=privileged # Create a debug pod with LVM tools and host /dev access # (chroot won't work on Talos \u2014 it has no shell on the host rootfs) kubectl apply -f - <<'EOF' apiVersion: v1 kind: Pod metadata: name: lvm-setup namespace: debug spec: hostPID: true containers: - name: lvm-setup image: ubuntu:24.04 command: [\"bash\", \"-c\", \"apt-get update && apt-get install -y lvm2 && echo READY && sleep 3600\"] securityContext: privileged: true volumeMounts: - name: dev mountPath: /dev - name: run-udev mountPath: /run/udev volumes: - name: dev hostPath: path: /dev - name: run-udev hostPath: path: /run/udev restartPolicy: Never EOF # Wait for lvm2 to be installed kubectl wait --for=condition=Ready pod/lvm-setup -n debug --timeout=180s # Create the LVM physical volume and volume group kubectl exec lvm-setup -n debug -- pvcreate /dev/sdb kubectl exec lvm-setup -n debug -- vgcreate lvmvg /dev/sdb # Clean up kubectl delete pod lvm-setup -n debug kubectl delete namespace debug The OpenEBS ArgoCD Application ( cluster/bootstrap/openebs.yaml ) will create the openebs-lvmpv StorageClass automatically.","title":"Storage HDD (OpenEBS LVM)"},{"location":"getting-started/#backup-hdd-minio","text":"The backup disk is mounted via the Talos patch cluster/patches/backup-disk.yml . Ensure the disk is formatted and the mount path matches: # Format the backup disk (if needed) via debug pod # mkfs.ext4 /dev/sdc # mkdir -p /var/minio-data # mount /dev/sdc /var/minio-data The Minio deployment ( cluster/system/minio/ ) uses a hostPath volume pointing to /var/minio-data .","title":"Backup HDD (Minio)"},{"location":"getting-started/#step-5-initialize-vault","text":"Vault must be initialized and unsealed manually on first boot: # Wait for Vault pod to be running kubectl get pods -n vault -w # Initialize Vault task vault-init # This outputs cluster-keys.json \u2014 SAVE THIS SECURELY # Unseal Vault (joins and unseals all replicas) task vault-unseal # Enable KV v1 secrets engine ROOT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") kubectl exec -n vault vault-0 -- sh -c \"VAULT_TOKEN=$ROOT_TOKEN vault secrets enable -path=kv -version=1 kv\" # Create the Vault token secret for External Secrets # (namespace may not exist yet \u2014 ApplicationSets haven't synced) kubectl create namespace external-secrets --dry-run=client -o yaml | kubectl apply -f - kubectl create secret generic vault-token \\ --namespace external-secrets \\ --from-literal=token=$ROOT_TOKEN","title":"Step 5: Initialize Vault"},{"location":"getting-started/#step-6-create-argocd-vault-credentials","text":"Create the vault-credentials secret so the AVP sidecars (already deployed via bootstrap) can connect to Vault: # Create the vault-credentials secret for AVP ROOT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") kubectl create secret generic vault-credentials \\ --namespace argocd \\ --from-literal=AVP_AUTH_TYPE=token \\ --from-literal=AVP_KV_VERSION=1 \\ --from-literal=AVP_TYPE=vault \\ --from-literal=VAULT_ADDR=http://vault.vault.svc.cluster.local:8200 \\ --from-literal=VAULT_TOKEN=$ROOT_TOKEN The ArgoCD repo-server is deployed with AVP CMP sidecars from bootstrap. Once this secret is created, the sidecars will start resolving <path:kv/...> placeholders in manifests at sync time.","title":"Step 6: Create ArgoCD Vault Credentials"},{"location":"getting-started/#step-7-populate-vault-secrets","text":"# Port-forward Vault task vault-port-forward & export VAULT_ADDR=http://localhost:8200 export VAULT_TOKEN=$(cat cluster-keys.json | jq -r \".root_token\") # Cluster basics vault kv put kv/cluster domain=\"yourdomain.com\" IP=\"192.168.1.100\" email=\"you@example.com\" # Cloudflare DNS token vault kv put kv/cloudflare dnsToken=\"your-cloudflare-api-token\" # Grafana vault kv put kv/grafana user=\"admin\" pass=\"your-grafana-password\" \\ prometheus_user=\"prometheus\" prometheus_pass=\"your-prom-password\" # Minio vault kv put kv/minio rootUser=\"minioadmin\" rootPassword=\"your-minio-password\" # Restic backup credentials (use Minio creds) vault kv put kv/restic \\ AWS_ACCESS_KEY_ID=\"minioadmin\" \\ AWS_SECRET_ACCESS_KEY=\"your-minio-password\" \\ RESTIC_PASSWORD=\"your-restic-encryption-password\" \\ RESTIC_REPOSITORY=\"s3:http://minio.minio.svc.cluster.local:9000/backups\" # --- App secrets (add when deploying apps) --- # Kyoo (media server) vault kv put kv/kyoo apikey=\"your-kyoo-api-key\" tmdb=\"your-tmdb-api-key\" # Sonarr vault kv put kv/sonarr \\ token=\"your-sonarr-api-key\" \\ url=\"http://sonarr.sonarr.svc.cluster.local:8989\" # Radarr vault kv put kv/radarr \\ token=\"your-radarr-api-key\" \\ url=\"http://radarr.radarr.svc.cluster.local:7878\"","title":"Step 7: Populate Vault Secrets"},{"location":"getting-started/#step-8-verify","text":"# Check cluster health task health # Check ArgoCD applications kubectl get applications -n argocd # Access ArgoCD UI (get initial admin password) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d # Then access via the ingress: https://argocd.<your-domain>","title":"Step 8: Verify"},{"location":"getting-started/#upgrading","text":"","title":"Upgrading"},{"location":"getting-started/#talos","text":"task upgrade-talos -- v1.10.0","title":"Talos"},{"location":"getting-started/#kubernetes","text":"task upgrade-k8s -- v1.33.0","title":"Kubernetes"},{"location":"secret-management/","text":"Secret Management To avoid storing secrets in the repository, we use Vault to store them (e.g., API keys, passwords, etc.). We then use the External Secrets operator to sync the secrets stored in Vault with the Kubernetes cluster. Vault Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. Vault provides a unified interface to any secret while providing tight access control and recording a detailed audit log. It is installed in the vault namespace using the ArgoCD application defined in cluster/bootstrap/vault.yaml . Initialize the Vault cluster Every time the Vault pod restarts, it must be unsealed. We use a single unseal key (acceptable for a homelab). kubectl exec -n vault vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json You obtain the root token in the cluster-keys.json file. Ensure to keep it safe and do not commit it to the repository (or encrypt it if you do). age -R ~/.ssh/id_ed25519.pub cluster-keys.json > cluster-keys.json.age # Encrypt the file age -d -i ~/.ssh/id_ed25519 cluster-keys.json.age > cluster-keys.json # Decrypt the file Now unseal Vault: export VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) kubectl exec -n vault -ti vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY You can also use the Taskfile shortcut: task vault-unseal . If the Vault pod restarts, you will need to unseal it again. Enable the KV secret engine Once Vault is ready, enable the KV v1 secret engine: kubectl port-forward -n vault svc/vault 8200:8200 & # Or use: task vault-port-forward export VAULT_ADDR=\"http://localhost:8200\" export VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) vault secrets enable -path=kv -version=1 kv To test the Vault cluster, we can write a secret in the KV secret engine. vault kv put kv/foo my-value=s3cr3t External Secrets External Secrets allows you to use secrets stored in external secret management systems like HashiCorp Vault. It is installed in the external-secrets namespace using the ArgoCD application in cluster/system/external-secret/ . Note: This is actually the root token, which is not recommended for production use. In a production environment, you should create a dedicated token with the appropriate policies. Create a secret with the Vault token This is already handled by the bootstrap process, but if needed manually: kubectl create secret generic vault-token \\ --namespace external-secrets \\ --from-literal=token=$(jq -r \".root_token\" cluster-keys.json) ClusterSecretStore The ClusterSecretStore is defined in cluster/system/external-secret/cluster-store.yaml and points to Vault. Test the External Secrets Create an External Secret that syncs the secret in Vault with the Kubernetes cluster cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1 kind: ExternalSecret metadata: name: vault-example namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: example-sync # Name of the Secret in the target namespace data: - secretKey: foobar remoteRef: key: foo property: my-value EOF ArgoCD Vault Plugin For many applications, the ArgoCD Vault Plugin is used to inline Vault secrets into any Kubernetes manifest \u2014 not just Secrets but also ConfigMaps, Deployments, Ingresses, etc. Create a secret with the Vault token for the Vault Plugin kubectl create secret generic vault-credentials \\ --namespace argocd \\ --from-literal=AVP_AUTH_TYPE=token \\ --from-literal=AVP_KV_VERSION=1 \\ --from-literal=AVP_TYPE=vault \\ --from-literal=VAULT_ADDR=http://vault.vault.svc.cluster.local:8200 \\ --from-literal=VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) After creating this secret, the ArgoCD repo-server AVP sidecar containers (deployed automatically via the bootstrap kustomization) will be able to connect to Vault and resolve <path:kv/...> placeholders. The AVP configuration is defined in cluster/argocd/vault-argocd/ , which patches the argocd-repo-server Deployment with AVP sidecar containers and the CMP plugin ConfigMap. Once everything is applied, you can use AVP placeholders in any manifest. For example, to hide the domain in an Ingress: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-app annotations: cert-manager.io/cluster-issuer: cloudflare spec: ingressClassName: traefik rules: - host: \"my-app.<path:kv/cluster#domain>\" http: paths: - path: / pathType: Prefix backend: service: name: my-app port: number: 80 tls: - hosts: - \"my-app.<path:kv/cluster#domain>\" secretName: my-app-tls The <path:kv/cluster#domain> placeholder is replaced at sync time by the ArgoCD Vault Plugin with the value from Vault.","title":"Secret Management"},{"location":"secret-management/#secret-management","text":"To avoid storing secrets in the repository, we use Vault to store them (e.g., API keys, passwords, etc.). We then use the External Secrets operator to sync the secrets stored in Vault with the Kubernetes cluster.","title":"Secret Management"},{"location":"secret-management/#vault","text":"Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. Vault provides a unified interface to any secret while providing tight access control and recording a detailed audit log. It is installed in the vault namespace using the ArgoCD application defined in cluster/bootstrap/vault.yaml .","title":"Vault"},{"location":"secret-management/#initialize-the-vault-cluster","text":"Every time the Vault pod restarts, it must be unsealed. We use a single unseal key (acceptable for a homelab). kubectl exec -n vault vault-0 -- vault operator init \\ -key-shares=1 \\ -key-threshold=1 \\ -format=json > cluster-keys.json You obtain the root token in the cluster-keys.json file. Ensure to keep it safe and do not commit it to the repository (or encrypt it if you do). age -R ~/.ssh/id_ed25519.pub cluster-keys.json > cluster-keys.json.age # Encrypt the file age -d -i ~/.ssh/id_ed25519 cluster-keys.json.age > cluster-keys.json # Decrypt the file Now unseal Vault: export VAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json) kubectl exec -n vault -ti vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY You can also use the Taskfile shortcut: task vault-unseal . If the Vault pod restarts, you will need to unseal it again.","title":"Initialize the Vault cluster"},{"location":"secret-management/#enable-the-kv-secret-engine","text":"Once Vault is ready, enable the KV v1 secret engine: kubectl port-forward -n vault svc/vault 8200:8200 & # Or use: task vault-port-forward export VAULT_ADDR=\"http://localhost:8200\" export VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) vault secrets enable -path=kv -version=1 kv To test the Vault cluster, we can write a secret in the KV secret engine. vault kv put kv/foo my-value=s3cr3t","title":"Enable the KV secret engine"},{"location":"secret-management/#external-secrets","text":"External Secrets allows you to use secrets stored in external secret management systems like HashiCorp Vault. It is installed in the external-secrets namespace using the ArgoCD application in cluster/system/external-secret/ . Note: This is actually the root token, which is not recommended for production use. In a production environment, you should create a dedicated token with the appropriate policies. Create a secret with the Vault token This is already handled by the bootstrap process, but if needed manually: kubectl create secret generic vault-token \\ --namespace external-secrets \\ --from-literal=token=$(jq -r \".root_token\" cluster-keys.json) ClusterSecretStore The ClusterSecretStore is defined in cluster/system/external-secret/cluster-store.yaml and points to Vault.","title":"External Secrets"},{"location":"secret-management/#test-the-external-secrets","text":"Create an External Secret that syncs the secret in Vault with the Kubernetes cluster cat <<EOF | kubectl apply -f - apiVersion: external-secrets.io/v1 kind: ExternalSecret metadata: name: vault-example namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-backend kind: ClusterSecretStore target: name: example-sync # Name of the Secret in the target namespace data: - secretKey: foobar remoteRef: key: foo property: my-value EOF","title":"Test the External Secrets"},{"location":"secret-management/#argocd-vault-plugin","text":"For many applications, the ArgoCD Vault Plugin is used to inline Vault secrets into any Kubernetes manifest \u2014 not just Secrets but also ConfigMaps, Deployments, Ingresses, etc. Create a secret with the Vault token for the Vault Plugin kubectl create secret generic vault-credentials \\ --namespace argocd \\ --from-literal=AVP_AUTH_TYPE=token \\ --from-literal=AVP_KV_VERSION=1 \\ --from-literal=AVP_TYPE=vault \\ --from-literal=VAULT_ADDR=http://vault.vault.svc.cluster.local:8200 \\ --from-literal=VAULT_TOKEN=$(jq -r \".root_token\" cluster-keys.json) After creating this secret, the ArgoCD repo-server AVP sidecar containers (deployed automatically via the bootstrap kustomization) will be able to connect to Vault and resolve <path:kv/...> placeholders. The AVP configuration is defined in cluster/argocd/vault-argocd/ , which patches the argocd-repo-server Deployment with AVP sidecar containers and the CMP plugin ConfigMap. Once everything is applied, you can use AVP placeholders in any manifest. For example, to hide the domain in an Ingress: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-app annotations: cert-manager.io/cluster-issuer: cloudflare spec: ingressClassName: traefik rules: - host: \"my-app.<path:kv/cluster#domain>\" http: paths: - path: / pathType: Prefix backend: service: name: my-app port: number: 80 tls: - hosts: - \"my-app.<path:kv/cluster#domain>\" secretName: my-app-tls The <path:kv/cluster#domain> placeholder is replaced at sync time by the ArgoCD Vault Plugin with the value from Vault.","title":"ArgoCD Vault Plugin"}]}